@inproceedings{dong2021,
  author    = {Dong, Yihe and Cordonnier, Jean-Baptiste and Loukas, Andreas},
  title     = {Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning (ICML)},
  year      = {2021},
}

@inproceedings{naitsaada2025,
  author    = {Nait Saada, Jad and others},
  title     = {Mind the Gap: A Spectral Analysis of Rank Collapse and Signal Propagation in Attention Layers},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2025},
}

@inproceedings{poole2016,
  author    = {Poole, Ben and Lahiri, Subhaneil and Raghu, Maithra and Sohl-Dickstein, Jascha and Ganguli, Surya},
  title     = {Exponential expressivity in deep neural networks through transient chaos},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2016},
}

@article{vogt2022,
  author    = {Vogt, Ryan and Puelma Touzel, Maximilian and Bhattacharjee, Einat and others},
  title     = {Lyapunov exponents for temporal networks},
  journal   = {arXiv preprint arXiv:2208.05089},
  year      = {2022},
}

@inproceedings{he2016,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  title     = {Deep residual learning for image recognition},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year      = {2016},
}

@inproceedings{tarnowski2019,
  author    = {Tarnowski, Wojciech and Warchol, Piotr and Jastrzebski, Stanislaw and Tabor, Jacek and Nowak, Maciej},
  title     = {Dynamical isometry is achieved in residual networks in a universal way for any activation function},
  booktitle = {International Conference on Artificial Intelligence and Statistics},
  year      = {2019},
}

@inproceedings{daneshmand2020,
  author    = {Daneshmand, Hadi and Kohler, Jonas and Bach, Francis and Hofmann, Thomas and Lucchi, Aurelien},
  title     = {Batch normalization provably avoids ranks collapse for randomly initialised deep networks},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2020},
}

@inproceedings{yang2019,
  author    = {Yang, Greg},
  title     = {Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2019},
}
